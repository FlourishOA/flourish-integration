{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports and scraper instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scrapers = [\\n    BioMedCentralScraper(\"https://www.biomedcentral.com/journals\"),\\n    ElsevierScraper(\"data/elsevier/2016-uncleaned.csv\"),\\n    ExistingScraper(\"data/OA_journals.tsv\"),\\n    HindawiScraper(\"http://www.hindawi.com/apc/\"),\\n    PLOSScraper(\"https://www.plos.org/publication-fees\"),\\n    SageHybridScraper(\"\"),\\n    SpringerHybridScraper(\"data/springer/2016+Springer+Journals+List.csv\"),\\n    SpringerOpenScraper(\"http://www.springeropen.com/journals\"),\\n    WileyScraper(\"http://olabout.wiley.com/WileyCDA/Section/id-828038.html\")\\n]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Scrapers.journalscrapers import BioMedCentralScraper, ElsevierScraper, ExistingScraper, \\\n",
    "    HindawiScraper, PLOSScraper, SageHybridScraper, SpringerHybridScraper, SpringerOpenScraper, \\\n",
    "    WileyScraper\n",
    "\n",
    "import json\n",
    "import csv\n",
    "\"\"\"scrapers = [\n",
    "    BioMedCentralScraper(\"https://www.biomedcentral.com/journals\"),\n",
    "    ElsevierScraper(\"data/elsevier/2016-uncleaned.csv\"),\n",
    "    ExistingScraper(\"data/OA_journals.tsv\"),\n",
    "    HindawiScraper(\"http://www.hindawi.com/apc/\"),\n",
    "    PLOSScraper(\"https://www.plos.org/publication-fees\"),\n",
    "    SageHybridScraper(\"\"),\n",
    "    SpringerHybridScraper(\"data/springer/2016+Springer+Journals+List.csv\"),\n",
    "    SpringerOpenScraper(\"http://www.springeropen.com/journals\"),\n",
    "    WileyScraper(\"http://olabout.wiley.com/WileyCDA/Section/id-828038.html\")\n",
    "]\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing output of scrapers to file for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/OA_journal_prices_2016.csv\", \"w\") as f:\n",
    "    for scraper in scrapers:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"publisher_name\", \"journal_name\", \"date_stamp\", \"is_hybrid\", \"issn\", \"apc\"])\n",
    "        try:\n",
    "            for i in scraper.get_entries():\n",
    "                writer.writerow([j.encode('utf8') if type(j) != bool else j for j in i])\n",
    "        except StopIteration:\n",
    "            print str(scraper) + \" isn't implemented yet.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Web Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from journalscrapers import BioMedCentralScraper, ElsevierScraper, ExistingScraper, \\\n",
    "    HindawiScraper, PLOSScraper, SageHybridScraper, SpringerHybridScraper, SpringerOpenScraper, \\\n",
    "    WileyScraper\n",
    "\n",
    "import json\n",
    "\n",
    "scrapers = [\n",
    "    #BioMedCentralScraper(\"https://www.biomedcentral.com/journals\"),\n",
    "    #ElsevierScraper(\"data/elsevier/2016-uncleaned.csv\"),\n",
    "    #ExistingScraper(\"data/OA_journals.tsv\"),\n",
    "    #HindawiScraper(\"http://www.hindawi.com/apc/\"),\n",
    "    #PLOSScraper(\"https://www.plos.org/publication-fees\"),\n",
    "    #SageHybridScraper(\"\"),\n",
    "    #SpringerHybridScraper(\"data/springer/2016+Springer+Journals+List.csv\"),\n",
    "    #SpringerOpenScraper(\"http://www.springeropen.com/journals\"),\n",
    "    #WileyScraper(\"http://olabout.wiley.com/WileyCDA/Section/id-828038.html\")\n",
    "]\n",
    "\n",
    "agg_journal_requests = []\n",
    "agg_price_requests = []\n",
    "for scraper in scrapers:\n",
    "    try:\n",
    "        for i in scraper.get_entries():\n",
    "            raw_data = dict(zip([\"pub\", \"name\", \"date_stamp\", \"is_hybrid\", \"issn\", \"apc\"], i))\n",
    "            print raw_data\n",
    "            agg_journal_requests.append({\n",
    "                'issn': raw_data['issn'],\n",
    "                'journal_name': raw_data['name'],\n",
    "                'pub_name': raw_data['pub'],\n",
    "                'is_hybrid': raw_data['is_hybrid'],\n",
    "            })\n",
    "\n",
    "            agg_price_requests.append({\n",
    "                'issn': raw_data['issn'],\n",
    "                'price': raw_data['apc'],\n",
    "                'date_stamp': raw_data['date_stamp'],\n",
    "            })\n",
    "    except StopIteration:\n",
    "        print str(scraper) + \" isn't implemented yet.\"\n",
    "\n",
    "\n",
    "with open(\"data/journal_requests.json\", \"a+\") as f:\n",
    "    f.write(json.dumps(agg_journal_requests))\n",
    "\n",
    "with open(\"data/price_requests.json\", \"r\") as f:\n",
    "    pl = json.load(f)\n",
    "\n",
    "with open(\"data/price_requests.json\", \"w\") as f:\n",
    "    f.write(json.dumps(pl + agg_price_requests))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Artricle Influence Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing requests into the main site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://52.175.212.92/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://localhost:8000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "r = requests.post(url + 'api-token-auth/',\n",
    "                  data={\n",
    "                      'username': '',\n",
    "                      'password': '',\n",
    "                  })\n",
    "token = \"Token \" + json.loads(r.text)['token']\n",
    "print token\n",
    "\n",
    "request_dicts = [\n",
    "    #{\"file_path\": \"category_update_requests.json\", \"url_stub\": \"api/journals/\"},\n",
    "    #{\"file_path\": \"journal_requests.json\", \"url_stub\": \"api/journals/\"},\n",
    "    #{\"file_path\": \"influence_requests.json\", \"url_stub\": \"api/influence/\"},\n",
    "    {\"file_path\": \"price_requests.json\", \"url_stub\": \"api/prices/\"},\n",
    "    ]\n",
    "valid_issn = set()\n",
    "for rd in request_dicts:\n",
    "    with open(\"data/\" + rd['file_path'], \"r\") as f:\n",
    "        for request_data in json.load(f):\n",
    "            if rd['file_path'] == \"journal_requests.json\":\n",
    "                valid_issn.add(request_data['issn'])\n",
    "            #elif request_data['issn'] in valid_issn:\n",
    "            if rd['file_path'] != \"influence_requests.json\" or request_data['issn'] in valid_issn:\n",
    "                #and rd['file_path'] != \"journal_requests.json\":\n",
    "                request = requests.put(\n",
    "                    url + rd['url_stub'] + request_data['issn'] + \"/\",\n",
    "                    headers={\n",
    "                        'Authorization': token,\n",
    "                        'Content-Type': 'application/json',\n",
    "                    },\n",
    "                    data=json.dumps(request_data),\n",
    "                )\n",
    "                print request_data\n",
    "                print request.status_code\n",
    "            else:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "\n",
    "\n",
    "web_url = 'http://52.175.212.92/'\n",
    "dev_url = 'http://localhost:8000/'\n",
    "url = web_url\n",
    "\n",
    "r = requests.post(url + 'api-token-auth/',\n",
    "                  data={\n",
    "                      'username': '',\n",
    "                      'password': ''\n",
    "                  })\n",
    "token = \"Token \" + json.loads(r.text)['token']\n",
    "print token\n",
    "with open(\"data/journal_requests.json\", \"r\") as f:\n",
    "    valid_issns = {journal['issn'] for journal in json.load(f)}\n",
    "\n",
    "with open(\"data/journals_EF_AI_2014.txt\", \"r\") as f:\n",
    "    client = requests.session()\n",
    "\n",
    "    for journal in csv.reader(f, dialect=csv.excel_tab):\n",
    "        if journal[1] in valid_issns and journal[23] != \"NULL\":\n",
    "            request_data = {\"issn\": journal[1], \"category\": journal[23]}\n",
    "\n",
    "            request = requests.put(\n",
    "                url + \"api/journals/\" + journal[1] + \"/\",\n",
    "                headers={\n",
    "                    'Authorization': token,\n",
    "                    'Content-Type': 'application/json',\n",
    "                },\n",
    "                data=json.dumps(request_data),\n",
    "            )\n",
    "            print request.status_code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the elsevier stuff in the data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from journalscrapers import ElsevierScraper\n",
    "import requests\n",
    "import json\n",
    "\n",
    "web_url = 'http://52.175.212.92/'\n",
    "dev_url = 'http://localhost:8000/'\n",
    "url = web_url\n",
    "r = requests.post(url + 'api-token-auth/',\n",
    "                  data={\n",
    "                      'username': '',\n",
    "                      'password': ''\n",
    "                  })\n",
    "token = \"Token \" + json.loads(r.text)['token']\n",
    "print token\n",
    "scraper = ElsevierScraper(\"data/elsevier/2016-uncleaned.csv\")\n",
    "\n",
    "agg_journal_requests = []\n",
    "agg_price_requests = []\n",
    "try:\n",
    "    for i in scraper.get_entries():\n",
    "        raw_data = dict(zip([\"pub\", \"name\", \"date_stamp\", \"is_hybrid\", \"issn\", \"apc\"], i))\n",
    "\n",
    "        request_data = {\n",
    "            'issn': raw_data['issn'],\n",
    "            'price': raw_data['apc'],\n",
    "            'date_stamp': raw_data['date_stamp'],\n",
    "        }\n",
    "        request = requests.put(\n",
    "            url + \"api/prices/\" + request_data['issn'] + \"/\",\n",
    "            headers={\n",
    "                'Authorization': token,\n",
    "                'Content-Type': 'application/json',\n",
    "            },\n",
    "            data=json.dumps(request_data),\n",
    "        )\n",
    "        print request.status_code\n",
    "except StopIteration:\n",
    "    print str(scraper) + \" isn't implemented yet.\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "integ",
   "language": "python",
   "name": "integ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
